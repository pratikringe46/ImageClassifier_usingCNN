{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#https://www.youtube.com/watch?v=Lakz2MoHy6o"
      ],
      "metadata": {
        "id": "_MaH6wUQujFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# from scipy import signal"
      ],
      "metadata": {
        "id": "yBJDa9XTrCgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def correlate2d_valid(input_matrix, kernel):\n",
        "    # Get the dimensions of the input matrix and the kernel\n",
        "    input_height, input_width = input_matrix.shape\n",
        "    kernel_height, kernel_width = kernel.shape\n",
        "\n",
        "    # Determine the output dimensions\n",
        "    output_height = input_height - kernel_height + 1\n",
        "    output_width = input_width - kernel_width + 1\n",
        "\n",
        "    output = np.zeros((output_height, output_width))\n",
        "\n",
        "    for i in range (output_height):\n",
        "        for j in range (output_width):\n",
        "            output[i,j] = np.sum(input_matrix[i:i+kernel_height, j:j+kernel_width] * kernel)\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "def convolve2d_full(input_matrix, kernel):\n",
        "    # Get the dimensions of the input matrix and the kernel\n",
        "    input_height, input_width = input_matrix.shape\n",
        "    kernel_height, kernel_width = kernel.shape\n",
        "\n",
        "    # Pad the input for full convolution\n",
        "    pad_height = kernel_height - 1\n",
        "    pad_width = kernel_width - 1\n",
        "    padded_input = np.pad(input_matrix, ((pad_height, pad_height), (pad_width, pad_width)), mode='constant')\n",
        "\n",
        "    kernel = np.rot90(kernel, 2)\n",
        "\n",
        "\n",
        "    output = correlate2d_valid(padded_input, kernel)\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "# Example below\n",
        "\n",
        "# input_mat = np.array([[3, 0, 1, 2, 7],\n",
        "#                          [1, 5, 8, 9, 3],\n",
        "#                          [2, 7, 2, 5, 1],\n",
        "#                          [0, 1, 3, 1, 6],\n",
        "#                          [4, 2, 1, 6, 2]], dtype = 'float')\n",
        "# kernel = np.array([[-1, 0, 1],\n",
        "#                          [-2, 0, 2],\n",
        "#                          [-1, 0, 1]], dtype = 'float')\n",
        "\n",
        "# mat1 = correlate2d_valid(input_mat, kernel, \"full\")\n",
        "# print(mat1)\n",
        "\n",
        "# print()\n",
        "\n",
        "# mat3 = convolve2d_full(input_mat, kernel)\n",
        "# print(mat3)"
      ],
      "metadata": {
        "id": "RvW0vXDmi6CK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer:\n",
        "  def __init__(self):\n",
        "    self.input=None\n",
        "    self.output=None\n",
        "\n",
        "  def forward_prop(self,input):\n",
        "    pass\n",
        "\n",
        "  def backward_prop(self,output_gradient,learning_rate):\n",
        "    pass"
      ],
      "metadata": {
        "id": "zoc0PwzUrEpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OKtA6NMp1ik"
      },
      "outputs": [],
      "source": [
        "class Convolutional(Layer):\n",
        "  def __init__(self,input_shape,kernel_size,num_kernels):\n",
        "    input_channels,input_height,input_width=input_shape #input_shape is a tuple\n",
        "    self.num_kernels=num_kernels                          #no of kernels/filters\n",
        "    self.input_shape=input_shape\n",
        "    self.input_channels=input_channels\n",
        "    self.output_shape=(num_kernels,input_height-kernel_size+1,input_width-kernel_size+1)\n",
        "    self.kernels_shape=(num_kernels,input_channels,kernel_size,kernel_size)\n",
        "\n",
        "\n",
        "    #filter values are initialized randomly with shape (depth, input_depth, kernel_size, kernel_size)\n",
        "    self.kernels=np.random.randn(*self.kernels_shape)\n",
        "\n",
        "    #bias is added to each output, initialized randomly with shape (depth, output_height, output_width)\n",
        "    self.biases=np.random.randn(*self.output_shape)\n",
        "\n",
        "  def forward_prop(self,inputs):\n",
        "    self.inputs=inputs\n",
        "    input_channels, input_height, input_width = self.inputs.shape\n",
        "    num_kernel, _, kernel_height, kernel_width = self.kernels.shape\n",
        "\n",
        "    output_height=input_height-kernel_height+1\n",
        "    output_width=input_width-kernel_width+1\n",
        "\n",
        "    self.feature_maps=np.copy(self.biases)\n",
        "\n",
        "    #convolution operation\n",
        "    for kernel_idx in range(self.num_kernels):\n",
        "      for channel_idx in range(self.input_channels):\n",
        "        self.feature_maps[kernel_idx] += correlate2d_valid(self.inputs[channel_idx], self.kernels[kernel_idx, channel_idx])\n",
        "\n",
        "    return self.feature_maps\n",
        "\n",
        "\n",
        "\n",
        "    def backward_prop(self,output_gradient,learning_rate):\n",
        "      kernels_gradient=np.zeros(self.kernels_shape)\n",
        "      inputs_gradient=np.zeros(self.inputs.shape)\n",
        "\n",
        "      input_channels,input_height,input_width=self.inputs.shape\n",
        "      num_kernel,_,kernel_height,kernel_width=self.kernels.shape\n",
        "      output_height=input_height-kernel_height+1\n",
        "      output_width=input_width-kernel_width+1\n",
        "\n",
        "\n",
        "      #compute input and kernel gradient\n",
        "      for kernel_idx in range(self.num_kernels):\n",
        "        for channel_idx in range(self.input_channels):\n",
        "          kernels_gradient[kernel_idx, channel_idx] = correlate2d_valid(self.inputs[channel_idx], output_gradient[kernel_idx])\n",
        "          inputs_gradient[channel_idx] += convolve2d_full(self.output_gradient[kernel_idx], self.kernels[kernel_idx, channel_idx])\n",
        "\n",
        "\n",
        "      #updating kernels and biases\n",
        "      self.kernels-=learning_rate*kernels_gradient\n",
        "      self.biases-=learning_rate*output_gradient\n",
        "\n",
        "      return inputs_gradient"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to reshape the output of the convolutional layer as the dense layer requires input to be a column vector\n",
        "\n",
        "class Reshape(Layer):\n",
        "  def __init__(self,input_shape,output_shape):\n",
        "    self.input_shape=input_shape\n",
        "    self.output_shape=output_shape\n",
        "\n",
        "  def forward_prop(self,input):\n",
        "    return np.reshape(input,self.output_shape)\n",
        "\n",
        "  def backward_prop(self,output_gradient,learning_rate):\n",
        "    return np.reshape(output_gradient,self.input_shape)\n"
      ],
      "metadata": {
        "id": "P2h6HSC2qbe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_cross_entropy(y_true,y_pred):\n",
        "  bce_loss=-np.mean(y_true*np.log(y_pred)+(1-y_true)*np.log(1-y_pred))\n",
        "  return bce_loss"
      ],
      "metadata": {
        "id": "yfHOZEgVtmuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#derivative of BCE loss function\n",
        "\n",
        "def D_binary_cross_entropy(y_true,y_pred):\n",
        "  D=((1-y_true)/(1-y_pred)-y_true/y_pred)/ np.size(y_true)\n",
        "  return D\n"
      ],
      "metadata": {
        "id": "9C-9XSGwyQaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#base class for activation functions\n",
        "\n",
        "class Activation(Layer):\n",
        "  def __init__(self,activation,activation_der):\n",
        "    self.activation=activation\n",
        "    self.activation_der=activation_der  #derivative of the activation\n",
        "\n",
        "  def forward_prop(self,input):\n",
        "    self.input=input\n",
        "    return self.activation(self.input)\n",
        "\n",
        "  #computes the gradient of loss wrt input\n",
        "  def backward_prop(self,output_gradient,learning_rate):\n",
        "    return np.multiply(output_gradient, self.activation_der(self.input))\n",
        "\n"
      ],
      "metadata": {
        "id": "CF_JsCRrz-WY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Sigmoid(Activation):\n",
        "  def __init__(self):\n",
        "\n",
        "    def sigmoid(y):\n",
        "      return 1/(1+ np.exp(-y))\n",
        "\n",
        "    def sigmoid_der(y):\n",
        "      s=sigmoid(y)\n",
        "      return s*(1-s)\n",
        "    #calls the constructor of the Activation(parent) class\n",
        "    #passess the sigmoid and its derivative to the Activation class\n",
        "    super().__init__(sigmoid,sigmoid_der)"
      ],
      "metadata": {
        "id": "X_gMj-7r07Gf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReLU(Activation):\n",
        "  def __init__(self):\n",
        "    def relu(x):\n",
        "      return np.maximum(0,x)\n",
        "\n",
        "    def relu_der(x):\n",
        "      return np.where(x>0,1,0)   #1 for positive values, 0 for negative\n",
        "\n",
        "    super().__init__(relu,relu_der)"
      ],
      "metadata": {
        "id": "tF2oN87GBc1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dense(Layer):\n",
        "  def __init__(self,input_size,output_size):\n",
        "    #assign random weights and biases\n",
        "    #they will be updated during back prop\n",
        "    self.weights=np.random.randn(output_size,input_size)\n",
        "    self.bias=np.random.randn(output_size,1)\n",
        "\n",
        "  def forward_prop(self,input):\n",
        "    self.input=input\n",
        "    M=np.dot(self.weights,self.input)+self.bias\n",
        "    return M\n",
        "\n",
        "  def backward_prop(self,output_gradient,learning_rate):\n",
        "     #dot product of output grad and input transpose\n",
        "    weights_gradient=np.dot(output_gradient,self.input.T)\n",
        "    inputs_gradient=np.dot(self.weights.T,output_gradient)\n",
        "\n",
        "    self.weights-=learning_rate*weights_gradient\n",
        "    self.bias-=learning_rate*output_gradient\n",
        "\n",
        "    return inputs_gradient\n"
      ],
      "metadata": {
        "id": "8cV-YeAR3Cr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#functions for training the model and predicting\n",
        "\n",
        "def predict(model,input):\n",
        "  output=input\n",
        "\n",
        "  for layer in model:\n",
        "    output=layer.forward_prop(output)\n",
        "    # print(\"Type of output: \",type(output))\n",
        "\n",
        "  return output"
      ],
      "metadata": {
        "id": "S203jQ7j49fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from re import X\n",
        "def train(model,x_train,y_train,loss,loss_der,epochs=1000,alpha=0.01,v=True):\n",
        "\n",
        "  for e in range(epochs):\n",
        "    err=0\n",
        "    #zip pairs each pair of input with its corresponding output\n",
        "    for x,y in zip(x_train,y_train):\n",
        "      output=predict(model,x)\n",
        "      # print(\"type is: \",type(output))\n",
        "\n",
        "      #compute the loss for this predicted output\n",
        "      err=err+loss(y,output)\n",
        "      #compute the gradient of the loss\n",
        "      grad=loss_der(y,output)\n",
        "\n",
        "      #iterate the network in reverse order\n",
        "      for layer in reversed(model):\n",
        "        grad=layer.backward_prop(grad,alpha)  #returns the grad wrt input of the layer\n",
        "\n",
        "    err=err/len(x_train)  #compute the average error for the epoch\n",
        "\n",
        "    if v:\n",
        "      print(f\"{e+1}/{ epochs}, error={err}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "pc9A8tIY6FHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "def preprocess_data(x, y, limit):\n",
        "    zero_index = np.where(y == 0)[0][:limit]\n",
        "    one_index = np.where(y == 1)[0][:limit]\n",
        "    all_indices = np.hstack((zero_index, one_index))\n",
        "    all_indices = np.random.permutation(all_indices)\n",
        "    x, y = x[all_indices], y[all_indices]\n",
        "    x = x.reshape(len(x), 1, 28, 28)\n",
        "    x = x.astype(\"float32\") / 255\n",
        "    y = to_categorical(y)\n",
        "    y = y.reshape(len(y), 2, 1)\n",
        "    return x, y\n",
        "\n",
        "# load MNIST from server, limit to 100 images per class since we're not training on GPU\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, y_train = preprocess_data(x_train, y_train, 100)\n",
        "x_test, y_test = preprocess_data(x_test, y_test, 100)\n",
        "\n",
        "# print(\"Shape\",x_train.shape)\n",
        "# print(\"type\",type(x_train))\n",
        "\n",
        "# neural network\n",
        "network = [\n",
        "    Convolutional((1, 28, 28), 3, 5),\n",
        "    Sigmoid(),\n",
        "    Reshape((5, 26, 26), (5 * 26 * 26, 1)),\n",
        "    Dense(5 * 26 * 26, 100),\n",
        "    Sigmoid(),\n",
        "    Dense(100, 2),\n",
        "    Sigmoid()\n",
        "]\n",
        "\n",
        "# train\n",
        "train(\n",
        "    network,\n",
        "    x_train,\n",
        "    y_train,\n",
        "    binary_cross_entropy,\n",
        "    D_binary_cross_entropy,\n",
        "    epochs=20,\n",
        "    alpha=0.1\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "LMHaoyKP8awT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf7d43e7-ca7c-4764-8b6f-bd43933e3bfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/20, error=0.4308836679645987\n",
            "2/20, error=0.12005237344073479\n",
            "3/20, error=0.09095241957927268\n",
            "4/20, error=0.07788493535422468\n",
            "5/20, error=0.044867453761574\n",
            "6/20, error=0.030000899038268292\n",
            "7/20, error=0.0326328685384639\n",
            "8/20, error=0.023800431049711107\n",
            "9/20, error=0.031320266510846594\n",
            "10/20, error=0.02789341586784317\n",
            "11/20, error=0.012122814773818902\n",
            "12/20, error=0.013157293139597686\n",
            "13/20, error=0.010363976513047324\n",
            "14/20, error=0.007165678006629313\n",
            "15/20, error=0.007134751635606503\n",
            "16/20, error=0.0061012266617984446\n",
            "17/20, error=0.005059775107213872\n",
            "18/20, error=0.004599319544875391\n",
            "19/20, error=0.004248824096376183\n",
            "20/20, error=0.003900418529766483\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "correct = 0\n",
        "total= 0\n",
        "for x, y in zip(x_test, y_test):\n",
        "    output = predict(network, x)\n",
        "    # print(f\"pred: {np.argmax(output)}, true: {np.argmax(y)}\")\n",
        "    total += 1\n",
        "    if (np.argmax(output) == np.argmax(y)):\n",
        "        correct += 1\n",
        "\n",
        "print(f\"Accuracy: {correct / total * 100}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJ0aai-TDRUM",
        "outputId": "78e450f0-d009-4ed8-d18f-c91c31b579cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 99.5%\n"
          ]
        }
      ]
    }
  ]
}